{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fe3fd2-f037-4e65-a467-88757a4b2efc",
   "metadata": {},
   "source": [
    "# Jobflow hands-on\n",
    "\n",
    "Jobflow is a free, open-source library for writing and executing workflows. Complex workflows can be defined using simple python functions and executed **locally** or on arbitrary computing resources using the [jobflow-remote](https://matgenix.github.io/jobflow-remote/) or [FireWorks](https://materialsproject.github.io/fireworks/) workflow managers.\n",
    "\n",
    "In this hands-on, we will only execute the workflows locally. In subsequent sessions, jobflow-remote will be presented and used for the execution of the workflows.\n",
    "\n",
    "Topics covered in this session:\n",
    "- The @job decorator and the Job object\n",
    "- Local execution with run_locally\n",
    "- Creating flows\n",
    "- Dynamic workflows\n",
    "- Concept of Makers\n",
    "\n",
    "In addition to these general topics, a series of <font color=\"green\">**DO**</font>'s and <font color=\"red\">**DONT**</font>'s or common errors will be (on purpose) shown in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3e18a-0458-4b24-a98f-47825cf37bb3",
   "metadata": {},
   "source": [
    "## The @job decorator and the Job object\n",
    "\n",
    "Let's define a (very) simple function that adds two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69526f-120c-44b2-b2ea-0c45afc3076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a+b\n",
    "\n",
    "result = add(2, 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08eba8-fbff-4d44-80da-2730fe55ea0e",
   "metadata": {},
   "source": [
    "The output is simply the sum of the two numbers. Now let's use the @job decorator for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d300a-58f2-4b58-abad-21834d116ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import job\n",
    "\n",
    "@job\n",
    "def add(a, b):\n",
    "    return a+b\n",
    "\n",
    "add_job = add(2, 5)\n",
    "print(add_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558603ce-b185-4798-8941-5797f923c584",
   "metadata": {},
   "source": [
    "The function add now return a Job object. The sum of the two numbers has not yet been executed. The execution is deferred to a later time, i.e. either when we execute it locally or when a workflow manager executes it (after its submission). The deferred output can be accessed through the output attribute of the job. This output attribute contains an OutputReference, which is a \"link\" to the job output (or an attribute, item, subitem, ... of this output - more about this later!).\n",
    "\n",
    "A Job object has a unique identifier (uuid) and an index. The uuid is automatically assigned when the job is created. The index is by default 1 when the job is first created. We will see later on when (as well as why and how) this index can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ae348-e05a-49c6-9f5f-591ac8c1869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_job.uuid)   # This uniquely identifies a Job\n",
    "print(add_job.index)  # This is the index of a given Job (two or more jobs can have the same uuid but different - increasing - index's)\n",
    "print(add_job.output) # OutputReference - Link to this job output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba3673-7872-4c77-a0e6-4d0bd9029c2d",
   "metadata": {},
   "source": [
    "<font color=\"red\">**DONT**</font> Be careful with namespaces. Users might be tempted here to name the resulting job as \"job\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16393f7-eb41-4e75-9590-e35546262cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = add(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8922b-8d87-49d9-a45e-029e9ea78767",
   "metadata": {},
   "source": [
    "Now you can't decorate another function because \"job\" has been redefined..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ee500-f7f1-48c9-86b2-eb191f465f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@job\n",
    "def somefunction(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0d3ba-4708-474d-afaf-9c17c262b1fd",
   "metadata": {},
   "source": [
    "## Local execution with run_locally\n",
    "Let's now reimport job here and recreate some jobs to be executed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f56de-0595-44b3-80f6-03ef3d9992ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import job, run_locally\n",
    "\n",
    "@job\n",
    "def add(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7c12f-9ee5-4a97-82a4-7729d2eb3247",
   "metadata": {},
   "source": [
    "Let's create one job and run it locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206da92-1688-4016-bf15-ff144b5fda2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_job = add(2, 3)\n",
    "output = run_locally(add_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b8ea6-6471-4c44-976f-b516fa59988e",
   "metadata": {},
   "source": [
    "We some logging about the job starting and finishing.\n",
    "\n",
    "And now look at the output. We are using pprint here to get a nicely printed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca9caa-eadc-4c4a-82bc-5f877e6c619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0ca57-2188-4d44-8a7a-4add06979a05",
   "metadata": {},
   "source": [
    "What do we have in here ?\n",
    "- A dictionary with the uuid of the job as a key\n",
    "- The value for that key is another dictionary\n",
    "- This other dictionary has the index of the job as a key\n",
    "- The value for that index key is a Response object\n",
    "- The Response object contains\n",
    "  - The output of the original add function (i.e. the sum of the two numbers - here 2 + 3 = ... drumroll ... 5)\n",
    "  - Other stuff (see later)\n",
    "\n",
    "<font color=\"red\">**DONT**</font> run the same job twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c4370-a3ac-4bfd-bf44-9aa3be687694",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_locally(add_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37236947-520d-4e37-a513-93a7d2bbc359",
   "metadata": {},
   "source": [
    "The above error seems quite cryptic, we will see why this happens later but in any case this happens because we tried to run this add_job twice.\n",
    "\n",
    "Where do the jobs run ? Let's see with another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26ca1b-82f4-4e25-810a-bb972f591b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "@job\n",
    "def hello(name):\n",
    "    \"\"\"This job writes a hello.txt file with Hello + the name.\"\"\"\n",
    "    Path('hello.txt').write_text(f'Hello {name}')\n",
    "\n",
    "hello_david = hello('David')\n",
    "hello_john = hello('John')\n",
    "\n",
    "run_locally(hello_david)\n",
    "run_locally(hello_john)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44228f-b1fb-43e4-9f5c-0e9b6865be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls\n",
    "!cat hello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8912b-be38-4d1c-a225-bcf2b907a877",
   "metadata": {},
   "source": [
    "The jobs have run in same directory (the current directory) and the second job (\"hello_john\") has overwritten the file produced by the first one.\n",
    "\n",
    "<font color=\"green\">**DO**</font> use create_folders=True in run_locally. This will create one folder for each job so that there is no clash of outputs and no lost data. Note that this is of course important when jobs create files (which is almost always the case) but in some cases, they do not, as in the example above with the *add* job function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57979475-22ae-4927-99df-f5d4f994186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_david = hello('David')\n",
    "hello_john = hello('John')\n",
    "\n",
    "run_locally(hello_david, create_folders=True)\n",
    "run_locally(hello_john, create_folders=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40f09d-7b80-4d6c-83ee-582fa04ecc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls\n",
    "!cat hello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524a232-ab73-4277-9473-7ee49fd1e1ff",
   "metadata": {},
   "source": [
    "You can look inside the two *job_2025-**** directories to see what's in there.\n",
    "\n",
    "Note that when you will start using Jobflow-Remote as a manager instead of the local manager (i.e. run_locally), you don't need to set anything like that as all job directories are automatically created by Jobflow-Remote. The local manager is very useful for training, testing, verifying, developing but once you are in production, you should use Jobflow-Remote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385a26b-d82b-4a68-a6f6-0b936245890b",
   "metadata": {},
   "source": [
    "## Creating Flows\n",
    "Flows are created easily from a list of jobs and/or flows.\n",
    "The dependencies between the different jobs and flows are automatically defined from the *OutputReference* links (output attributes of other jobs in arguments of a given job).\n",
    "The graph of jobs in a Flow can easily be visualized with matplotlib using the *draw_graph* method of the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26e9ca-d849-4fbc-bace-8c82a7f1ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import Flow\n",
    "\n",
    "@job\n",
    "def add(a, b):\n",
    "    return a+b\n",
    "\n",
    "@job\n",
    "def multiply(a, b):\n",
    "    return a*b\n",
    "\n",
    "\n",
    "add_job1 = add(2, 3)\n",
    "add_job2 = add(3, 5)\n",
    "multiply_job = multiply(add_job1.output, add_job2.output)\n",
    "\n",
    "myflow = Flow([add_job1, add_job2, multiply_job])\n",
    "\n",
    "myflow.draw_graph().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677870c9-ab02-4762-b797-bd201321e46c",
   "metadata": {},
   "source": [
    "Alternatively, you can also visualize the graph using mermaid. Generate the mermaid syntax using the *to_mermaid* function and paste the output on [Mermaid Live](https://mermaid.live/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab50d38-de28-47d3-84cb-1c1e2e43335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow.utils.graph import to_mermaid\n",
    "print(to_mermaid(myflow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956b200-51f3-4c4a-a415-9a6c98ff4010",
   "metadata": {},
   "source": [
    "Let's run this flow now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff0f0b-fd86-45db-94eb-eec162cdce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = run_locally(myflow, create_folders=True)\n",
    "pprint.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53fff2-4cfc-4cb0-8b3b-abdf651753a9",
   "metadata": {},
   "source": [
    "As for the case of running single jobs, we see some logging of the jobs being executed. They are (of course ...) being executed in the correct order, i.e. multiply can only be executed after the two add jobs have finished.\n",
    "\n",
    "Try to figure out what is the final result here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079e9b6-149a-4736-8470-df5a68f9fb0a",
   "metadata": {},
   "source": [
    "## Dynamic workflows\n",
    "\n",
    "Jobs can create other jobs (and flows). This is done by using the *Response* object as a returned value of a job instead of \"just\" its output. In addition to an *output* argument (which should contain the output of the job), the *Response* object has other arguments used for dealing with dynamic updates of the Flow:\n",
    "- detour\n",
    "- addition\n",
    "- replace\n",
    "- ...\n",
    "\n",
    "Here we will give an example of using an *addition* or a *replace* for calculating the Fibonacci series. The Fibonacci series is a sequence of numbers where each number is the sum of the two preceding ones. It starts with 0 and 1, and the sequence goes like this:\n",
    "\n",
    "    0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ...\n",
    "\n",
    "The following code allows to print the Fibonacci series with a recursion method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5870c-cc7e-460e-acfb-52fba24445b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(smaller, larger, maximum=50):\n",
    "    total = smaller + larger\n",
    "    if total > maximum:\n",
    "        print(total)\n",
    "        return\n",
    "\n",
    "    print(f'{total}, ', end=\"\")\n",
    "    return fibonacci(larger, total, maximum=maximum)\n",
    "\n",
    "fibonacci(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c8674-727f-4450-b75f-a84e1b5bf624",
   "metadata": {},
   "source": [
    "Now let's adapt this to use a dynamic flow to compute the series of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a130c858-c4c7-430b-8a31-b5e069d7b8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from jobflow import Response\n",
    "\n",
    "@job\n",
    "def fibonacci(smaller, larger, maximum=50):\n",
    "    total = smaller + larger\n",
    "    if total > maximum:\n",
    "        return total\n",
    "\n",
    "    new_job = fibonacci(larger, total, maximum=maximum)\n",
    "    return Response(output=total, addition=new_job)\n",
    "\n",
    "fibonacci_job = fibonacci(0, 1)\n",
    "outputs = run_locally(fibonacci_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733730b7-592a-40b2-af36-3984a7c235a9",
   "metadata": {},
   "source": [
    "Although there was only one job at the beginning, multiple jobs have been executed. Let's look at the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6351e2-1996-4fb9-9916-ecdc3b1f4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2b31e-976c-4e0f-aaaf-dc3e7c12d052",
   "metadata": {},
   "source": [
    "Difficult to know the order of the jobs and get back the series... Each job that sums two numbers has its own uuid and these are not ordered. Let's modify things a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba51a2-48b4-493e-9e3f-046c52a66ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import Response\n",
    "\n",
    "@job\n",
    "def fibonacci(smaller, larger, index=0, maximum=50):\n",
    "    total = smaller + larger\n",
    "    if total > maximum:\n",
    "        return {\"index\": index, \"number\": total}\n",
    "\n",
    "    new_job = fibonacci(larger, total, index=index+1, maximum=maximum)\n",
    "    return Response(output={\"index\": index, \"number\": total}, addition=new_job)\n",
    "\n",
    "fibonacci_job = fibonacci(0, 1)\n",
    "outputs = run_locally(fibonacci_job)\n",
    "pprint.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ff151-e7f5-4f44-93e3-1244cfb89ae8",
   "metadata": {},
   "source": [
    "Now at least we can go through it in an ordered manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d1a5c-8d22-4581-a434-504955c41def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for jobs_with_same_uuid in sorted(outputs.items(), key=lambda kv: kv[1][1].output['index']):\n",
    "    print(jobs_with_same_uuid[1][1].output['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a2906-4ac9-4154-9c68-d9a53eb67939",
   "metadata": {},
   "source": [
    "Instead of using an *addition*, we can also use a *replace*, and now the index of the jobs increases but all jobs have the same uuid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203185f6-f22b-48ce-8bd5-04c7821f03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import Response\n",
    "\n",
    "@job\n",
    "def fibonacci(smaller, larger, maximum=50):\n",
    "    total = smaller + larger\n",
    "    if total > maximum:\n",
    "        return total\n",
    "\n",
    "    new_job = fibonacci(larger, total, maximum=maximum)\n",
    "    return Response(output=total, replace=new_job)\n",
    "\n",
    "fibonacci_job = fibonacci(0, 1)\n",
    "outputs = run_locally(fibonacci_job)\n",
    "pprint.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f4a7a-e469-46ef-b846-e3db6be299aa",
   "metadata": {},
   "source": [
    "## The concept of Makers\n",
    "\n",
    "The general concept of Makers in jobflow is to define \"global\" or \"general\" parameters of the procedure/flow/workflow that will be applied to all the calculations and separate these from the specific parameters for each calculation. For example, you might consider that the accuracy, the k-point density and the exchange-correlation functional are general parameters, while the structure, the charge, maybe some atoms or cells constraints are specific parameters. This concept is very important and allows to ensure reproducibility of calculations. You can share makers (makers are *serializable* and can be dumped as a json file) with other users/colleagues so that everyone uses the exact same general parameters.\n",
    "\n",
    "Let's look at this concept for a very simplified toy example. Let's define \"something\" that increments a number. The goal is to have a job that, when given a number, will increment this number by a given amount and return the result. You could imagine that the increment amount is a general parameter (i.e. everytime you pass a number, you want to increment it by 10), and the number itself is the specific parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af30168-c01c-433e-b8ac-f7d96ba4068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from jobflow import Maker\n",
    "\n",
    "@dataclass\n",
    "class IncrementMaker(Maker):\n",
    "    name: str = \"increment\"\n",
    "    increment: int = 1\n",
    "\n",
    "    @job\n",
    "    def make(self, number):\n",
    "        return number + self.increment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c7207-bf74-44a7-8713-e9e56663cd3b",
   "metadata": {},
   "source": [
    "Now you can initialize this increment maker (with the default value of 1 for the increment) and generate different jobs with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13130559-ba2d-4094-baa4-bddfa3c869f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_maker_1 = IncrementMaker()  # Will use the default value of 1 for the increment\n",
    "job1 = increment_maker_1.make(5)      # Will be incremented by 1 => 6\n",
    "job2 = increment_maker_1.make(12)      # Will be incremented by 1 => 13\n",
    "job3 = increment_maker_1.make(500)      # Will be incremented by 1 => 501\n",
    "\n",
    "pprint.pprint(run_locally(job1))\n",
    "pprint.pprint(run_locally(job2))\n",
    "pprint.pprint(run_locally(job3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40f7a3-924e-46cf-b0bf-110b0f1fe118",
   "metadata": {},
   "source": [
    "We can initialize the maker with a different value for the increment amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32265dd7-0e4d-4fdb-921a-e3256078a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_maker_5 = IncrementMaker(increment=5)  # Will use the default value of 1 for the increment\n",
    "job1 = increment_maker_5.make(5)        # Will be incremented by 5 => 10\n",
    "job2 = increment_maker_5.make(12)       # Will be incremented by 5 => 17\n",
    "job3 = increment_maker_5.make(500)      # Will be incremented by 5 => 505\n",
    "\n",
    "pprint.pprint(run_locally(job1))\n",
    "pprint.pprint(run_locally(job2))\n",
    "pprint.pprint(run_locally(job3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702af4ba-76d9-4557-88a4-b8d80f3e59dc",
   "metadata": {},
   "source": [
    "You can pretty print makers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dac275-1bb3-4ee8-a248-5fa42e0c1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(increment_maker_1)\n",
    "print(increment_maker_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255b926-21ef-4259-a003-9c345a1ddaf6",
   "metadata": {},
   "source": [
    "You can easily dump these makers to a json file and share them with colleagues (or store them for later reuse). Note here that this will work if the implemented maker is importable! If you implement a Maker in a script and serialize/dump it to a file, it can only be recreated if the actual Maker code is also importable on the machine where you would deserialize it. Within this notebook session, this will work as the code is still in memory but when you use Jobflow-Remote, this won't work anymore! It will of course work for atomate2 Makers as these are importable both locally (i.e. within this notebook) and anywhere else you would have atomate2 installed. \\[see Jobflow-Remote and atomate2 hands-ons for more details\\]\n",
    "\n",
    "For convenience, we use *dumpfn* from the *monty* package to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba507f-ce80-4c4f-a3ca-c28f37fa0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monty.serialization import dumpfn\n",
    "\n",
    "dumpfn(increment_maker_1, 'maker_1.json', indent=2)  # here we use indent=2 to have a json file that is pretty printed\n",
    "dumpfn(increment_maker_5, 'maker_5.json', indent=2)  # here we use indent=2 to have a json file that is pretty printed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad446748-e080-48e9-ac3d-422cb5fbe3b9",
   "metadata": {},
   "source": [
    "You can look at the generated files to see what's in there.\n",
    "\n",
    "Let's deserialize one of them (we use the *loadfn* helper function from *monty* for that) and recreate a few jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42331d51-2681-4165-87ef-49537ba6f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monty.serialization import loadfn\n",
    "\n",
    "some_maker = loadfn('maker_5.json')\n",
    "job1 = some_maker.make(15)\n",
    "job2 = some_maker.make(30)\n",
    "\n",
    "pprint.pprint(run_locally(job1))\n",
    "pprint.pprint(run_locally(job2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e6c87-df6b-4a28-8efe-1487a6cfc9a4",
   "metadata": {},
   "source": [
    "## Accessing results using queries\n",
    "\n",
    "Until now, the default store has been used for run_locally, i.e. a MemoryStore. Once run_locally has finished its execution, it does not exist anymore and you can't access the results anymore (unless you have assigned to an outputs variable for example). Now we will use a predefined store for running some calculations and see how we can access the outputs programmatically directly from the database.\n",
    "\n",
    "First, we initialize the JobStore. A pre-configured store can be instantiated from a file. Note that this is only used in this first hands-on. Please note that in subsequent hands-ons and in production, the store is defined in the jobflow-remote config directly and we strongly advice you to use such a setup for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2b97d-cb16-4859-a79e-4a38cb367b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobflow import JobStore\n",
    "store = JobStore.from_file('config/my_store.yaml')\n",
    "\n",
    "# Must connect to the store before being able to use\n",
    "store.connect()\n",
    "# Let's erase the database first (comment if you want to keep)\n",
    "store.remove_docs({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea669c-8886-40c7-9e77-5004a9fc0a98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Let's recreate a fibonnacci job and run it locally with this store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7889a36-f063-452f-a135-eb6c7898d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacci_job = fibonacci(0, 1)\n",
    "run_locally(fibonacci_job, store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48242007-a0a9-4f97-833a-a2a8eb6440a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now we can get the outputs directly from that store. We can see how many job outputs are stored. Let's also have a look at one (random) output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318b27-2b95-4eee-bfd5-afb59ece1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store.count())\n",
    "pprint.pprint(store.query_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086dd010-67a9-4e51-ab73-498d0b6bb7e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We can get all the numbers from the fibonacci series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6ca7b-9307-4380-be54-7e3623629dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in store.query():\n",
    "    print(doc['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90adf4-c989-4612-b8d7-5523a3d24dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Let's make another double add and multiply flow as in the beginning of this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c05fe-1a75-493a-8ee1-dca93c3ce123",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_job1 = add(1, 2)\n",
    "add_job2 = add(3, 3)\n",
    "multiply_job = multiply(add_job1.output, add_job2.output)\n",
    "\n",
    "myflow = Flow([add_job1, add_job2, multiply_job])\n",
    "run_locally(myflow, store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db8747-59cb-4cfa-b793-c170fa034b4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now the three outputs are in the store as well. We can get the result of the multiply job with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01d73d-6ef5-4dee-88c3-32b540124a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store.count())\n",
    "print(\"Result of the multiplication: \", store.query_one({\"name\": \"multiply\"})[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
